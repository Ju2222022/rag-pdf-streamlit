import os
import fitz  # PyMuPDF
import faiss
import numpy as np
import streamlit as st
import torch
from tempfile import NamedTemporaryFile
from sentence_transformers import SentenceTransformer

# === CONFIG ===
EMBED_MODEL = "all-MiniLM-L6-v2"
model = SentenceTransformer(EMBED_MODEL)
model.to(torch.device("cpu"))  # ğŸ§  Forcer l'utilisation du CPU

def pdf_to_chunks(pdf_file, chunk_size=500):
    doc = fitz.open(pdf_file)
    full_text = "\n".join([page.get_text() for page in doc])
    chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
    return chunks

def embed_chunks(chunks):
    return model.encode(chunks)

def build_faiss_index(embeddings):
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings).astype("float32"))
    return index

st.title("ğŸ“˜ Assistant conformitÃ© (multi-PDF + IA)")
st.write("Chargez un ou plusieurs fichiers PDF rÃ©glementaires, posez une question, et obtenez une rÃ©ponse basÃ©e sur leur contenu.")

uploaded_files = st.file_uploader("ğŸ“¤ Chargez un ou plusieurs fichiers PDF", type="pdf", accept_multiple_files=True)

if uploaded_files:
    all_chunks = []

    for f in uploaded_files:
        with NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(f.read())
            tmp_path = tmp.name

        st.info(f"ğŸ“„ Lecture de : {f.name}")
        chunks = pdf_to_chunks(tmp_path)
        all_chunks.extend(chunks)

    st.success(f"âœ… {len(uploaded_files)} fichiers lus. {len(all_chunks)} morceaux extraits.")

    embeddings = embed_chunks(all_chunks)
    index = build_faiss_index(np.array(embeddings))

    question = st.text_input("â“ Posez votre question liÃ©e aux documents :")

    if question:
        question_vec = model.encode([question])
        D, I = index.search(np.array(question_vec).astype("float32"), k=3)
        context = "\n---\n".join([all_chunks[i] for i in I[0]])
        
        st.markdown("### ğŸ§  Contexte extrait :")
        st.write(context)

        st.markdown("âš ï¸ Cette version ne gÃ©nÃ¨re pas encore de rÃ©ponse rÃ©sumÃ©e avec un LLM. Souhaitez-vous quâ€™on lâ€™ajoute ?")

import json

# === Sauvegarde de lâ€™index FAISS ===
faiss.write_index(index, "index.faiss")
st.success("ğŸ’¾ Index FAISS sauvegardÃ© sous 'index.faiss'.")

# === Sauvegarde des textes correspondants ===
with open("chunks.json", "w", encoding="utf-8") as f:
    json.dump(all_chunks, f, ensure_ascii=False, indent=2)
st.success("ğŸ’¾ Texte sauvegardÃ© sous 'chunks.json'.")


